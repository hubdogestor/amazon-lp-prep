// Case 4 - bias_for_action
const case_4 = {
  id: "calculated-risk-time-critical",
  title: "Risco Calculado em Tempo Crítico: Restaurei Telemedicina em 72h com Mitigação por Zonas e Rollback Seletivo",
  title_pt: "Risco Calculado em Tempo Crítico: Restaurei Telemedicina em 72h com Mitigação por Zonas e Rollback Seletivo",
  title_en: "Calculated Risk Under Time Pressure: Restored Telemedicine in 72h with Zoned Mitigation and Selective Rollback",
  company: "Unimed Porto Alegre",
  period: "03/2023-05/2023",
  isTopCase: true,
  isGoodCase: false,
  pt: {
    s: `Situacao: "38 mil consultas em risco. Custo de R$ 1,2 milhao por dia offline. Infraestrutura propoe duas semanas de rebuild. Pacientes onco sem atendimento." Em 3 de marco de 2023 as 6h37 da manha sofremos ataque ransomware devastador que derrubou completamente a plataforma de telemedicina e colocou 38 mil consultas agendadas em risco imediato de cancelamento. O time de infraestrutura imediatamente propunha reinstalar todo o ambiente do zero em duas semanas de trabalho, seguindo protocolo conservador de seguranca. Cada dia offline custaria R$ 1,2 milhao em glosas contratuais com operadoras, multas regulatorias da ANS e perda de receita direta. Mais critico ainda: pacientes oncologicos e cardiopatas criticos ficariam sem atendimento especializado em momento de vulnerabilidade maxima. Eu tinha apenas algumas horas para decidir entre aceitar duas semanas de caos ou assumir risco calculado de restauracao acelerada com potencial de instabilidade tecnica e vazamento de dados pessoais.`,
    t: `Tarefa: O desafio critico que enfrentei foi reestabelecer no minimo 80% da capacidade operacional da telemedicina em apenas 72 horas, mantendo conformidade rigorosa com LGPD e garantindo capacidade de rollback imediato caso o ambiente restaurado ficasse instavel ou apresentasse brecha de seguranca. Como head de transformacao digital eu era owner direto do produto de telemedicina e de todas as integracoes criticas com prontuario, billing e prescricao digital. Minha missao pessoal nao era apenas restaurar tecnologia rapidamente - era salvar 38 mil consultas de pacientes vulneraveis, evitar R$ 8,4 milhoes em perdas financeiras acumuladas em uma semana, manter NPS acima de 70 mesmo em crise, e fazer tudo isso sem criar nenhum incidente de privacidade ou vazamento de dados sensiveis de saude que poderia destruir confianca de 15 anos construida com cooperados. Eu sabia que cada hora de indecisao custaria vidas, dinheiro e reputacao simultaneamente.`,
    a: `Acao: Minha abordagem foi estruturar um mecanismo de resposta chamado plano "Phoenix" baseado em arquitetura por zonas isoladas, rollback seletivo ensaiado e comunicacao transparente continua com stakeholders criticos. Primeiro eu estruturei o plano "Phoenix" dividindo a plataforma monolitica em cinco zonas operacionais independentes: consulta agendamento, billing faturamento, prontuario eletronico, video conferencia e prescricoes digitais. Em apenas duas horas convoquei conselho de crise presencial com CFO, CIO, juridico e DPO, propus migrar zona critica de consultas para cloud publica temporaria da AWS com isolamento de rede, e assumi responsabilidade pessoal total pelo cutover arriscado. Depois negociei duramente com juridico e DPO para conseguir autorizacao emergencial de 72 horas respaldada por parecer formal documentando salvaguardas tecnicas e juridicas. Em seguida importei backup offline das ultimas 12 horas para storage isolado criptografado e escrevi pessoalmente scripts de sanitizacao que rodaram varredura completa em 26 minutos eliminando qualquer vestigio de ransomware. Eu criei runbook detalhado de rollback para cada zona com RPO de 15 minutos e RTO de 90 minutos, defini criterios objetivos de acionamento automatico (latencia >450ms, erro HTTP >2% por 5 min), organizei tres squads dedicados 24/7 (infra, produto, seguranca) e coordenei pessoalmente war-room ininterrupta. Para proteger experiencia dos medicos cooperados abri canal direto via bot com 120 medicos mais ativos recebendo push a cada 30 minutos e permitindo reporte instantaneo de falhas. Quando a zona de video apresentou latencia critica de 480 ms no segundo dia, eu acionei rollback seletivo em apenas sete minutos, migrei para provedor alternativo pre-homologado e retomei servico estavel em 31 minutos sem perder nenhuma consulta em andamento. Por fim publiquei boletim tecnico transparente a cada duas horas para SAC, reguladores ANS e imprensa, evitando escalonamento de midia e mantendo confianca publica durante a crise.`,
    r: `Resultado: O impacto mensuravel desse esforco coordenado foi a restauracao mais rapida da historia da cooperativa e a preservacao de confianca em momento de maxima vulnerabilidade. Em apenas 61 horas restabeleci 83% da capacidade operacional completa com SLA de consulta reduzido para 12 minutos (antes do ataque era 18 minutos - ficamos ate mais rapidos pos-crise). Evitamos 32 mil cancelamentos de consultas que teriam gerado exodo de cooperados e perda irreparavel de confianca, mantivemos NPS acima de 71 pontos mesmo durante a crise (benchmark de crise e <50), e reduzimos perdas financeiras potenciais em R$ 5,8 milhoes comparado ao cenario de duas semanas offline. Nenhum incidente de privacidade ou vazamento de dados foi registrado pela auditoria interna ou pela ANS, validando as salvaguardas de LGPD implementadas sob pressao. Atendimento oncologico critico voltou em apenas 36 horas, permitindo continuidade de tratamentos quimioterapicos sem interrupcao perigosa. O plano Phoenix foi imediatamente institucionalizado como protocolo oficial de resposta a incidentes da cooperativa e replicado com sucesso em 9 regionais Unimed do Sul e Sudeste em programas de disaster recovery.`,
    l: `Licao: A licao principal desse caso transformou minha abordagem a gestao de incidentes criticos sob pressao de tempo e risco. Aprendi que enfrentar incidentes criticos com velocidade e seguranca simultaneas exige tres pilares tecnicos inegociaveis: (1) Arquitetura por zonas isoladas - nunca mais construo sistema core monolitico, sempre divido em blocos que podem falhar ou ser restaurados independentemente sem derrubar tudo (Phoenix provou que zona de video podia cair sem impactar consultas); (2) Rollback ensaiado com criterios objetivos - nao basta ter plano B em papel, preciso de gatilhos automaticos calibrados (latencia >450ms, erro >2% por 5min) e runbooks testados trimestralmente em simulacao real de cutover, nao apenas teorica; (3) Comunicacao franca e continua - em crise, silencio mata confianca mais rapido que falha tecnica, entao publico boletim a cada 2 horas mesmo sem novidade, mantenho canal direto com usuarios criticos via bot, e documento tudo para reguladores antes que perguntem. Hoje qualquer sistema core que lidero ja nasce com matriz Phoenix embutida na arquitetura - cinco zonas minimas, rollback automatico por zona, simulacao trimestral obrigatoria de disaster recovery - e nao aprovo go-live sem essas salvaguardas validadas. Esse kit de resiliencia ja evitou dois incidentes posteriores onde rollback automatico acionou em 4 e 9 minutos respectivamente, salvando SLA sem precisar de guerra manual.`,
  },
  en: {
    s: `Situation: "38 thousand appointments at risk. Cost of BRL 1.2 million per day offline. Infrastructure proposes two-week rebuild. Oncology patients without care." On March 3rd 2023 at 6:37am we suffered devastating ransomware attack that completely shut down the telemedicine platform and put 38 thousand scheduled appointments at immediate cancellation risk. Infrastructure team immediately proposed reinstalling entire environment from scratch in two weeks of work, following conservative security protocol. Each offline day would cost BRL 1.2 million in contractual chargebacks with insurers, ANS regulatory fines and direct revenue loss. More critical still: oncology and critical cardiac patients would be left without specialized care at moment of maximum vulnerability. I had only a few hours to decide between accepting two weeks of chaos or taking calculated risk of accelerated restoration with potential for technical instability and personal data leak.`,
    t: `Task: The critical challenge I faced was reestablishing at least 80% of telemedicine operational capacity in only 72 hours, maintaining strict LGPD compliance and guaranteeing immediate rollback capability if restored environment became unstable or presented security breach. As head of digital transformation I directly owned the telemedicine product and all critical integrations with medical records, billing and digital prescription. My personal mission wasn't just restoring technology quickly - it was saving 38 thousand appointments of vulnerable patients, avoiding BRL 8.4 million in accumulated financial losses over one week, maintaining NPS above 70 even in crisis, and doing all this without creating any privacy incident or leak of sensitive health data that could destroy 15 years of built trust with members. I knew each hour of indecision would cost lives, money and reputation simultaneously.`,
    a: `Action: My approach was structuring response mechanism called "Phoenix" plan based on isolated zone architecture, rehearsed selective rollback and continuous transparent communication with critical stakeholders. First I structured the "Phoenix" plan dividing monolithic platform into five independent operational zones: appointment scheduling, billing invoicing, electronic medical records, video conferencing and digital prescriptions. In only two hours I convened in-person crisis council with CFO, CIO, legal and DPO, proposed migrating critical appointment zone to temporary AWS public cloud with network isolation, and took total personal responsibility for risky cutover. Then I negotiated hard with legal and DPO to get 72-hour emergency authorization backed by formal opinion documenting technical and legal safeguards. Next I imported last 12-hour offline backup to encrypted isolated storage and personally wrote sanitization scripts that ran complete scan in 26 minutes eliminating any ransomware traces. I created detailed rollback runbook for each zone with 15-minute RPO and 90-minute RTO, defined objective automatic trigger criteria (latency >450ms, HTTP error >2% for 5 min), organized three dedicated 24/7 squads (infra, product, security) and personally coordinated uninterrupted war-room. To protect physician member experience I opened direct bot channel with 120 most active physicians receiving push every 30 minutes and allowing instant failure reporting. When video zone presented critical 480 ms latency on second day, I triggered selective rollback in only seven minutes, migrated to pre-approved alternate provider and restored stable service in 31 minutes without losing any ongoing appointment. Finally I published transparent technical bulletin every two hours to customer service, ANS regulators and press, avoiding media escalation and maintaining public trust during crisis.`,
    r: `Result: The measurable impact of this coordinated effort was the cooperative's fastest restoration in history and trust preservation at moment of maximum vulnerability. In only 61 hours I reestablished 83% of complete operational capacity with appointment SLA reduced to 12 minutes (before attack it was 18 minutes - we became even faster post-crisis). We avoided 32 thousand appointment cancellations that would have generated member exodus and irreparable trust loss, maintained NPS above 71 points even during crisis (crisis benchmark is <50), and reduced potential financial losses by BRL 5.8 million compared to two-week offline scenario. No privacy incident or data leak was recorded by internal audit or ANS, validating LGPD safeguards implemented under pressure. Critical oncology care returned in only 36 hours, allowing chemotherapy treatment continuity without dangerous interruption. The Phoenix plan was immediately institutionalized as official incident response protocol of cooperative and successfully replicated in 9 Unimed regional units in South and Southeast in disaster recovery programs.`,
    l: `Learning: The main lesson from this case transformed my approach to critical incident management under time pressure and risk. I learned that facing critical incidents with simultaneous speed and safety requires three non-negotiable technical pillars: (1) Isolated zone architecture - never again building monolithic core system, always divide into blocks that can fail or be restored independently without bringing everything down (Phoenix proved video zone could fail without impacting appointments); (2) Rehearsed rollback with objective criteria - having plan B on paper isn't enough, I need calibrated automatic triggers (latency >450ms, error >2% for 5min) and quarterly-tested runbooks in real cutover simulation, not just theoretical; (3) Frank continuous communication - in crisis, silence kills trust faster than technical failure, so I publish bulletin every 2 hours even without news, maintain direct channel with critical users via bot, and document everything for regulators before they ask. Today any core system I lead is born with Phoenix matrix embedded in architecture - five minimum zones, automatic rollback per zone, mandatory quarterly disaster recovery simulation - and I don't approve go-live without these validated safeguards. This resilience kit already prevented two subsequent incidents where automatic rollback triggered in 4 and 9 minutes respectively, saving SLA without needing manual war.`,
  },
  fups: [
    { q: "Como voce decidiu pelas cinco zonas do plano Phoenix?", a: "Eu mapeei dependencias e escolhi blocos que podiam operar isolados sem quebrar LGPD.", q_en: "How did you define the five Phoenix zones?", a_en: "I mapped dependencies and picked blocks that could run isolated without breaching LGPD." },
    { q: "Quais dados voce usou para conseguir aprovacao do DPO?", a: "Eu apresentei analise de impacto de R$1,2 mi por dia e as salvaguardas de criptografia e auditoria.", q_en: "Which data convinced the DPO?", a_en: "I shared the BRL 1.2M daily impact and the encryption and audit safeguards." },
    { q: "Como voce garantiu higiene dos backups em 26 minutos?", a: "Eu escrevi scripts de sanitizacao com hash duplo e bloqueio de executavel suspeito.", q_en: "How did you sanitise backups in 26 minutes?", a_en: "I built sanitisation scripts with double hashing and suspicious executable blocking." },
    { q: "Que criterios acionaram rollback seletivo da zona de video?", a: "Latencia acima de 450 ms e erro HTTP maior que 2% por 5 minutos.", q_en: "What triggered the selective rollback of the video zone?", a_en: "Latency above 450 ms and HTTP error over 2% for five minutes." },
    { q: "Como voce manteve os medicos informados?", a: "Eu criei bot que enviava push a cada 30 minutos e abria ticket automatico em caso de falha.", q_en: "How did you keep physicians informed?", a_en: "I built a bot pushing updates every 30 minutes and auto-opening tickets on failure." },
    { q: "Quais indicadores voce monitorou no war-room?", a: "Eu acompanhava disponibilidade por zona, tempo de consulta, fila de prescricao e alertas de LGPD.", q_en: "Which indicators did you track in the war-room?", a_en: "I tracked zone availability, consult duration, prescription queue, and LGPD alerts." },
    { q: "Como voce quantificou a perda evitada de R$5,8 mi?", a: "Eu projetei glosas, multas regulatorias e custo de cancelamento por especialidade.", q_en: "How did you quantify the BRL 5.8M loss avoided?", a_en: "I modelled chargebacks, regulatory fines, and cancellation cost per specialty." },
    { q: "Que comunicacao voce fez com reguladores?", a: "Eu emiti boletins a cada 2 horas com status, salvaguardas e plano de retomada validado.", q_en: "What did you communicate to regulators?", a_en: "I sent two-hour reports detailing status, safeguards, and the validated recovery plan." },
    { q: "O que voce mudou nos sistemas apos o incidente?", a: "Eu implantei zonamento permanente, backup imutavel e simulacao trimestral do Phoenix.", q_en: "What system changes followed the incident?", a_en: "I implemented permanent zoning, immutable backup, and quarterly Phoenix drills." },
    { q: "Qual aprendizado levou para outros projetos?", a: "Eu passei a exigir planilha Phoenix antes de qualquer go-live critico de TI.", q_en: "What lesson did you apply elsewhere?", a_en: "I now demand the Phoenix sheet before any critical IT go-live." }
  ]
};

export default case_4;
